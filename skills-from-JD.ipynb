{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T07:42:38.960614400Z",
     "start_time": "2023-07-11T07:41:10.149930Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# from spacy.matcher import PhraseMatcher\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# from skillNer.general_params import SKILL_DB\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# from skillNer.skill_extractor_class import SkillExtractor\u001B[39;00m\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\__init__.py:13\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# These are imported as part of the API\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mthinc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipeline  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m util\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabout\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\pipeline\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mattributeruler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AttributeRuler\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdep_parser\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DependencyParser\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01medit_tree_lemmatizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EditTreeLemmatizer\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mentity_linker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EntityLinker\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\pipeline\\dep_parser.pyx:1\u001B[0m, in \u001B[0;36minit spacy.pipeline.dep_parser\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:1\u001B[0m, in \u001B[0;36minit spacy.pipeline.transition_parser\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\ml\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_models_with_nvtx_range  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F401, F403\u001B[39;00m\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\ml\\models\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mentity_linker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmulti_task\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparser\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\ml\\models\\entity_linker.py:18\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mthinc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Floats2d\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Errors\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     19\u001B[0m     Candidate,\n\u001B[0;32m     20\u001B[0m     InMemoryLookupKB,\n\u001B[0;32m     21\u001B[0m     KnowledgeBase,\n\u001B[0;32m     22\u001B[0m     get_candidates,\n\u001B[0;32m     23\u001B[0m     get_candidates_batch,\n\u001B[0;32m     24\u001B[0m )\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokens\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Doc, Span\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m registry\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\kb\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcandidate\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Candidate, get_candidates, get_candidates_batch\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KnowledgeBase\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkb_in_memory\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InMemoryLookupKB\n",
      "File \u001B[1;32mD:\\EY\\venv\\lib\\site-packages\\spacy\\kb\\candidate.pyx:1\u001B[0m, in \u001B[0;36minit spacy.kb.candidate\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:398\u001B[0m, in \u001B[0;36mparent\u001B[1;34m(self)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "# from skillNer.general_params import SKILL_DB\n",
    "# from skillNer.skill_extractor_class import SkillExtractor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T07:43:11.631012800Z",
     "start_time": "2023-07-11T07:42:38.960614400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"D:\\EY\\LinkedIn-Job-Scraper\\output100.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jd_col = df['Job Description']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# All of NLP Preprocessing\n",
    "\n",
    "'''\n",
    "Removing of Special Characters\n",
    "    |\n",
    "    v\n",
    "Tokenization\n",
    "    |\n",
    "    v\n",
    "Stop Words Removal\n",
    "    |\n",
    "    v\n",
    "Filtering & lowering\n",
    "    |\n",
    "    v\n",
    "Joining the processed tokens\n",
    "'''\n",
    "\n",
    "def preprocess_job_description(job_description):\n",
    "\n",
    "    # Removing Special characters & numbers\n",
    "    job_description = re.sub(r'[^a-zA-Z]', ' ', job_description)\n",
    "\n",
    "    job_description = re.sub(r'\\d+\\.?\\d*', '',job_description)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(job_description)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Lower cases all of the JD\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Making the tokens into a proper string\n",
    "\n",
    "\n",
    "\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jd_col_list = jd_col.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_list = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for element in jd_col_list:\n",
    "    process = preprocess_job_description(str(element))\n",
    "    processed_list.append(process)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "def extract_skills_huggingface(jd):\n",
    "    MAX_LENGTH = 512\n",
    "    # Split the input string into chunks of maximum length\n",
    "    chunks = [jd[i:i+MAX_LENGTH] for i in range(0, len(jd), MAX_LENGTH)]\n",
    "    skl = []\n",
    "    for chunk in chunks:\n",
    "        response = requests.post(\"https://jjzha-skill-extraction-demo.hf.space/run/predict\", json={\n",
    "            \"data\": [\n",
    "                chunk\n",
    "            ]\n",
    "        }).json()\n",
    "\n",
    "        data = response[\"data\"]\n",
    "\n",
    "        for i in data:\n",
    "            for sk in i:\n",
    "                if sk[1] == 'Skill':\n",
    "                    skl.append(sk[0])\n",
    "\n",
    "    return skl\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "skill = []\n",
    "for jd in processed_list:\n",
    "    #print(type(jd))\n",
    "    skills = extract_skills_huggingface(jd)\n",
    "    skill.append(skills)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "skill"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import openpyxl\n",
    "#\n",
    "# def put_list_of_lists_in_excel(list_of_lists, excel_file):\n",
    "#     \"\"\"\n",
    "#     Puts a list of lists into different rows of an Excel file.\n",
    "#\n",
    "#     Args:\n",
    "#         list_of_lists: The list of lists to be inserted.\n",
    "#         excel_file: The path to the Excel file.\n",
    "#     \"\"\"\n",
    "#\n",
    "#     wb = openpyxl.Workbook()\n",
    "#     sheet = wb.active\n",
    "#\n",
    "#     row = 1\n",
    "#     for list_data in list_of_lists:\n",
    "#         for value in list_data:\n",
    "#             sheet.cell(row=row, column=1).value = value\n",
    "#             row += 1\n",
    "#\n",
    "#     wb.save(excel_file)\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     list_of_lists = skill\n",
    "#     excel_file = \"D:\\EY\\LinkedIn-Job-Scraper\\output100.xlsx\"\n",
    "#\n",
    "#     put_list_of_lists_in_excel(list_of_lists, excel_file)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['skills'] = skill"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the DataFrame back to the Excel file\n",
    "df.to_excel('D:\\EY\\LinkedIn-Job-Scraper\\output_skills10.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
