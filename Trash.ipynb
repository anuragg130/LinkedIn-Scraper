{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    "job india among top ten priority markets general mills hosts global shared services centre global shared services arm general mills inc supports operations worldwide employees mumbai center capabilities areas supply chain finance hr digital technology sales capabilities consumer insights itq r quality enterprise business services learning capacity building key ingredient success job overview passionate food driven data want impact future data analytics innovative technology thrive leading big things making happen bring passion expertise problem solving skills table make impact general mills reshaping future technology data play important role us technology experience help us get right data solutions right time every time one world leading food companies general mills operates across globe recognizable consumer brands one general mills key technology priorities driving business action connected data individual help drive data strategy processes ensure solutions sustainable follow general mills standards role requires ability manage multiple initiatives work well various internal external partners bring capabilities life across variety general mills teams must master language grammar familiar structure design implementation testing project based one languages role requires work cross functional global teams developing business solutions individual expert multiple technology stack hands design architect scalable applications individual able provide solutions architecture design independently large projects job responsibilities time design solutions integrate various downstream upstream systems applications design logical physical data models support business requirement continuously improve quality consistency accessibility security data activity define best practices define monitor orchestrate data quality business critical data understand data sources used c f develop plan enable improve integrations centralization maintenance able tie data integrations business processes day day operations develop adaptable data integrated solutions scale evolving business needs understand data sources systems integrated integrated exposure experience salesforce platform build connect complex systems architectures consult projects offer appropriate technical direction appropriate communication various audiences resource estimates identify elements project track effectively pull plans together improve project trajectory build enterprise level solutions connecting multiple feature sets components identifies opportunities integration points kafka elasticsearch etc lead strategic conversations act resource managers directors translate technical discussions responsible technical deliverables per design architecture evangelize new technologies emerging trends able efficiently find applicable uses collaborate teams utilize new features platform technologies time act key data analytics technical leader deep technical expertise collaboratively develop technology capability roadmap participate evaluation implementation deployment emerging tools process big data space partner business analysts solutions architects develop technical architectures strategic projects initiatives lead design implementation sustainable tools processes support big data ecosystem develop communication education plans technical teams technologies processes big data ecosystem participate active member assigned product team including supporting existing future solutions time learning mindset developing self others coaching mentoring go person others team define best practices process project improvements collaborates team utilize new features platform technologies consults projects lead discoveries projects technology networks cross functional industry experts advises project managers analysts architecture impacts strategies contribute help effective project task estimation strategic conversation go person managers directors technical discussions decisions desired profile education minimum degree requirements bachelors preferred degree requirements bachelors preferred major area study computer science experience minimum years related experience required years preferred years related experience years specific job experience skills needed domain expertise experience modern cloud platforms preferably google cloud hands programming experience one scala python java experience working database systems strong sql experience developing complex sqls ad hoc reporting experience hadoop toolsets including hive sqoop hdfs commands spark experience working directly business clients design solution meets business requirements ability clearly articulate pros cons various technologies platforms architectural options well able document use cases solutions recommendationso used general software engineering tools processes like git jenkins intellij eclipse bash terminal jira progress tracking agile etl reporting tools data governance data warehousing structured unstructured data effective verbal written communication influencing skills effective analytical technical skills ability work team environment ability research plan organize lead implement new processes technology architectural knowledge implementation microservices devops containerization jenkins pipeline etc experience integrating saas based solutions enterprise level experience database systems knowledge sql nosql stores e g mysql oracle mongodb sql server collaborate cross functional teams infra security qa across organization develop design solutions technical business problems connects individual efforts broad standards strategies agile digital experience expert task sprint estimation creates manages product feature roadmap aligns technology direction new standards architectures obsolescence helps drive agile journey team terms team velocity self organization effective retrospective conversations hadoop toolsets hdfs map reduce must hive sqoop spark must scala python java must sql plsql must big data modelling must etl reporting tools data warehousing data governance must cloud gcp must git jenkins jira agile must ecommerce marketing must data analytical solutions spark must sales force preffered kafka must competencies behaviors required job individual skills strong communication skills ability communicate complex technical concepts align organization decisions sound problem solving skills ability quickly process complex information present clearly simply utilizes team collaboration create innovative solutions efficiently mindset behaviors passionate technology excited impact emerging disruptive technologies open learning new ideas outside scope knowledge skillset creating positive environment within team ability research plan organize lead implement new processes technology challenge status quo company overview exist make food world loves company place prioritizes force good place expand learning explore new perspectives reimagine new possibilities every day look people want bring best bold thinkers big hearts challenge one grow together becoming undisputed leader food means surrounding people hungry next\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T14:06:04.731917300Z",
     "start_time": "2023-07-09T14:06:04.706994300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_skills_huggingface(jd):\n",
    "    MAX_LENGTH = 512\n",
    "\n",
    "    # Split the input string into chunks of maximum length\n",
    "    chunks = [jd[i:i+MAX_LENGTH] for i in range(0, len(jd), MAX_LENGTH)]\n",
    "\n",
    "    skl = []\n",
    "    for chunk in chunks:\n",
    "        response = requests.post(\"https://jjzha-skill-extraction-demo.hf.space/run/predict\", json={\n",
    "            \"data\": [\n",
    "                chunk\n",
    "            ]\n",
    "        }).json()\n",
    "\n",
    "        data = response[\"data\"]\n",
    "\n",
    "        for i in data:\n",
    "            for sk in i:\n",
    "                if sk[1] == 'Skill':\n",
    "                    skl.append(sk[0])\n",
    "\n",
    "    return skl\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T14:10:41.487629600Z",
     "start_time": "2023-07-09T14:10:41.468002600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def execute_function(string):\n",
    "\tchunk_size = 512\n",
    "\n",
    "\tchunks = [string[i:i+chunk_size] for i in range(0, len(string), chunk_size)]\n",
    "\n",
    "\tfor chunk in chunks:\n",
    "\t\tans = extract_skills_huggingface(chunk)\n",
    "\n",
    "\treturn ans"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T14:08:05.464586300Z",
     "start_time": "2023-07-09T14:08:05.416153600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "ans = extract_skills_huggingface(job_description)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T14:11:12.161913800Z",
     "start_time": "2023-07-09T14:10:50.817743400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problem solving skills', 'reshaping future technology data', 'driving business action', 'drive data strategy processes ensure solutions', 'manage multiple initiatives work', 'work cross functional global teams developing business solutions', 'hands design architect scalable applications', 'provide solutions architecture design independently large projects job', 'time design solutions integrate various downstream upstream systems applications design logical physical data models support business requirement continuously improve quality consistency accessibility security data activity define best practices define monitor orchestrate data quality business critical data understand data sources used c f develop plan enable improve integrations centralization maintenance able tie data integrations business processes', 'develop adaptable data', 'integrated solutions scale evolving business needs understand data sources systems', 'build connect complex systems architectures consult projects offer appropriate technical direction appropriate communication various audiences resource estimates identify elements project track effectively pull plans', 'improve project trajectory build enterprise level solutions', 'multiple feature sets components identifies opportunities integration', 'lead strategic conversations', 'resource', 'translate technical discussions responsible technical deliverables', 'evangelize new technologies emerging trends', 'collaborate teams utilize new features platform technologies time act key data analytics', 'leader', 'collaboratively develop technology capability roadmap', 'process big data space partner business analysts solutions architects develop technical architectures strategic projects initiatives lead design implementation sustainable tools processes support big data ecosystem develop communication education plans technical teams technologies processes big data ecosystem', 'supporting existing future solutions time learning mindset developing self others coaching mentoring', 'define best pract', 'ices process project improvements collaborates team utilize new features platform technologies consults projects lead discoveries projects technology networks', 'advises', 'contribute help effective project task estimation strategic conversation', 'directors technical discussions decisions', 'working database', 'developing complex sqls', 'working directly business clients design', 'clearly articulate pros cons various technologies platforms architectural options', 'document use cases solutions', 'structured unstructured data', 'influencing skills', 'skills', 'organize lead implement new processes', 'saas based solutions', 'develop design solutions technical business problems connects individual efforts', 'product feature roadmap', 'communication skills', 'communicate complex technical concepts align organization decisions', 'problem solving skills', 'process complex information present clearly simply utilizes team collaboration create innovative solutions efficiently mindset behaviors passionate', 'open learning new ideas outsid', 'creating positive environment', 'organize lead implement new processes', 'new']\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T14:11:14.679907500Z",
     "start_time": "2023-07-09T14:11:14.648689200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\"https://api-inference.huggingface.co/models/jjzha/jobbert_skill_extraction\",headers= {\"Authorization\": \"Bearer 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\"}, json={\n",
    "\t\"data\": [\n",
    "\t\tjob_description ,\n",
    "\t]\n",
    "}).json()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T11:24:49.836580300Z",
     "start_time": "2023-07-09T11:24:48.058206200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'At least one input is required.', 'warnings': ['There was an inference error: At least one input is required.']}\n"
     ]
    }
   ],
   "source": [
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T11:24:17.289748900Z",
     "start_time": "2023-07-09T11:24:17.274082700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdata\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T09:36:35.845512500Z",
     "start_time": "2023-07-09T09:36:35.782892200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data[0][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the Excel file\n",
    "data = pd.read_excel('skills_data.xlsx')\n",
    "\n",
    "# Extract the skills column\n",
    "skills = data['Skills']\n",
    "\n",
    "# Join all the skills into a single string\n",
    "all_skills = ' '.join(skills.astype(str))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(all_skills)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T05:31:58.773585Z",
     "start_time": "2023-07-10T05:31:41.189733700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.Series([1,2,3,4,5,6])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T05:32:15.454895100Z",
     "start_time": "2023-07-10T05:32:15.439118900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
